{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "import torch.nn.functional as func\n",
    "from PIL import Image, ImageDraw\n",
    "from skimage.measure import compare_ssim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "roi = 'v1.v2.v3'\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "lr = 1e-4\n",
    "wd = 1e-5\n",
    "interval = 100\n",
    "dropout_p = .0\n",
    "NUM_CLASSES = 8\n",
    "NUM_ROUTING_ITERATIONS = 3\n",
    "label = 'all ' + roi + ' wd:' + str(wd)\n",
    "device = torch.device('cuda')\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = f.relu(x)\n",
    "        # x = f.dropout(x, p=dropout_p, training=self.training)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = f.relu(x)\n",
    "        # x = f.dropout(x, p=dropout_p, training=self.training)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = f.relu(x)\n",
    "        # x = f.dropout(x, p=dropout_p, training=self.training)\n",
    "        # x = self.bn3(x)\n",
    "        x = self.fc4(x)\n",
    "        # x = x.view(-1, 8, 16)\n",
    "        # x = self.squash(x)\n",
    "        # x = x.view(-1, 8 * 16)\n",
    "        return x\n",
    "\n",
    "    def squash(self, tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(tensor, dim=-1):\n",
    "    squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * tensor / torch.sqrt(squared_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        l2_reg = None\n",
    "        for name, param in model.named_parameters():\n",
    "            if not (name == 'fc3.bias' or name == 'fc3.weight'):\n",
    "                if l2_reg is None:\n",
    "                    l2_reg = 0.5 * param.norm(2)**2\n",
    "                else:\n",
    "                    l2_reg += 0.5 * param.norm(2)**2\n",
    "        loss += l2_reg * wd\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % (interval/10) == 1:\n",
    "        writer.add_scalar('loss/train', loss.data.item(), epoch)\n",
    "        print('Epoch: {} Train Loss: {:.6f}'.format(epoch, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "\n",
    "    if epoch % (interval/10) == 1:\n",
    "        writer.add_scalar('loss/valid', val_loss, epoch)\n",
    "        print('Epoch: {} Valid Loss: {:.6f}'.format(epoch, val_loss))\n",
    "    return output, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_on_train(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data, target in train_for_validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(input, dim=1):\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = func.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels, kernel_size=None, stride=None,\n",
    "                 num_iterations=NUM_ROUTING_ITERATIONS):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "\n",
    "        self.num_route_nodes = num_route_nodes\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        if num_route_nodes != -1:\n",
    "            self.route_weights = nn.Parameter(torch.randn(num_capsules, num_route_nodes, in_channels, out_channels))\n",
    "        else:\n",
    "            self.capsules = nn.ModuleList(\n",
    "                [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0) for _ in\n",
    "                 range(num_capsules)])\n",
    "\n",
    "    def squash(self, tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_route_nodes != -1:\n",
    "            priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n",
    "\n",
    "            logits = Variable(torch.zeros(*priors.size())).to(device)\n",
    "            for i in range(self.num_iterations):\n",
    "                probs = softmax(logits, dim=2)\n",
    "                outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "\n",
    "                if i != self.num_iterations - 1:\n",
    "                    delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                    logits = logits + delta_logits\n",
    "        else:\n",
    "            outputs = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "            outputs = torch.cat(outputs, dim=-1)\n",
    "            outputs = self.squash(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1)\n",
    "        self.primary_capsules = CapsuleLayer(num_capsules=8, num_route_nodes=-1, in_channels=256, out_channels=32,\n",
    "                                             kernel_size=9, stride=2)\n",
    "        self.digit_capsules = CapsuleLayer(num_capsules=NUM_CLASSES, num_route_nodes=32 * 6 * 6, in_channels=8,\n",
    "                                           out_channels=16)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16 * NUM_CLASSES, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # x = func.relu(self.conv1(x), inplace=True)\n",
    "        # x = self.primary_capsules(x)\n",
    "        # x = self.digit_capsules(x).squeeze().transpose(0, 1)\n",
    "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
    "        classes = func.softmax(classes, dim=-1)\n",
    "        if y is None:\n",
    "            # In all batches, get the most active capsule.\n",
    "            _, max_length_indices = classes.max(dim=1)\n",
    "            y = Variable(torch.eye(NUM_CLASSES)).to(device).index_select(dim=0, index=max_length_indices.data)\n",
    "        reconstructions = self.decoder((x * y[:, :, None]).view(x.size(0), -1))\n",
    "        return classes, reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator():\n",
    "    data0 = []\n",
    "    labels = []\n",
    "    files = glob.glob('stim/*')\n",
    "    files = np.sort(files)\n",
    "    for i in range(0, len(files)):\n",
    "        im = Image.open(files[i])\n",
    "        im.load()\n",
    "        data0.append(np.asarray(im))\n",
    "        labels.append(i)\n",
    "    data0 = np.asarray(data0)\n",
    "    data0 = data0[:, 2:30, 2:30]\n",
    "    labels = np.asarray(labels)\n",
    "    data0 = np.expand_dims(data0, axis=1)\n",
    "    data = data0\n",
    "    for _ in range(7):\n",
    "        data = np.concatenate([data, data0], axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = ['sub02']\n",
    "sess = ['ses01']\n",
    "x = []\n",
    "for iSub in range(len(subs)):\n",
    "    for iSes in range(len(sess)):\n",
    "        sub = subs[iSub]\n",
    "        ses = sess[iSes]\n",
    "        x.append(np.genfromtxt('./probe/prob.' + sub + '.' + ses + '.' + roi + '.txt', delimiter=','))\n",
    "x = np.asarray(x)\n",
    "x = x.reshape((-1, 100))\n",
    "\n",
    "y0 = np.load('digitcaps.npy')\n",
    "y0 = y0.reshape((128, 8*16))\n",
    "y = y0\n",
    "for i in range(7):\n",
    "    y = np.concatenate([y, y0], axis=0)\n",
    "input_size = x.shape[1]\n",
    "\n",
    "writer = SummaryWriter(comment=label)\n",
    "rnd = np.random.permutation(len(x))\n",
    "x = x[rnd, :]\n",
    "y = y[rnd, :]\n",
    "\n",
    "x = x - 100\n",
    "train_num = int(len(x) * 0.75)\n",
    "valid_num = len(x) - train_num\n",
    "x_train = torch.from_numpy(x[:train_num, :])\n",
    "y_train = torch.from_numpy(y[:train_num, :])\n",
    "x_valid = torch.from_numpy(x[train_num:, :])\n",
    "y_valid = torch.from_numpy(y[train_num:, :])\n",
    "\n",
    "train_dataset = TensorDataset(x_train.float(), y_train.float())\n",
    "validation_dataset = TensorDataset(x_valid.float(), y_valid.float())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_for_validation_loader = DataLoader(dataset=train_dataset, batch_size=train_num, shuffle=False)\n",
    "validation_loader = DataLoader(dataset=validation_dataset, batch_size=valid_num, shuffle=False)\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n",
    "\n",
    "model = Net(input_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "capsnet = CapsuleNet().to(device)\n",
    "capsnet.load_state_dict(torch.load('stage1/epochs/epoch_500.pt'))\n",
    "\n",
    "rndTrainArg = np.argsort(rnd[:30])\n",
    "rndValidArg = np.argsort(rnd[-30:])\n",
    "\n",
    "ground_truth_valid = torch.tensor((get_iterator()[rnd[-30:], :, :]) / 255.0)\n",
    "ground_truth_train = torch.tensor((get_iterator()[rnd[:30], :, :]) / 255.0)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=500, verbose=False, filename='checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    outValid, val_loss = validation(epoch)\n",
    "    outTrain = validation_on_train(epoch)\n",
    "\n",
    "    # early_stopping(val_loss, model)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"Early stopping\")\n",
    "    #     break\n",
    "\n",
    "    if epoch % interval == 1:\n",
    "        outValid = outValid.reshape((valid_num, 8, 16))\n",
    "        outValid = outValid[-30:, :, :]\n",
    "        reconstructions = capsnet.forward(torch.tensor(squash(outValid)).to(device))[1]\n",
    "        reconstruction = reconstructions.cpu().view_as(ground_truth_valid).data\n",
    "        temp = torch.cat((ground_truth_valid[rndValidArg, :, :].double(), reconstruction[rndValidArg, :, :].double()), 0)\n",
    "        temp = torch.cat([temp, temp, temp], 1)\n",
    "        temp[:, 0, 27, :] = torch.max(temp)\n",
    "        temp[:, 1, 27, :] = 0\n",
    "        temp[:, 2, 27, :] = 0\n",
    "        grid = make_grid(temp, nrow=30, normalize=False, range=(0, 1)).numpy()\n",
    "        writer.add_image('Valid', grid, epoch)\n",
    "\n",
    "        # SSIM\n",
    "        A = ground_truth_valid.float().cpu().detach().numpy()\n",
    "        B = reconstruction.float().cpu().detach().numpy()\n",
    "        ssim = []\n",
    "        for i in range(len(ground_truth_valid)):\n",
    "            ssim.append(compare_ssim(A[i, 0, :, :], B[i, 0, :, :]))\n",
    "        ssimArgSort = np.argsort(ssim)\n",
    "\n",
    "        writer.add_scalar('ssim/valid', np.mean(ssim), epoch)\n",
    "        writer.add_scalar('ssimMax/valid', np.max(ssim), epoch)\n",
    "        temp = torch.cat((ground_truth_valid[ssimArgSort, :, :].double(), reconstruction[ssimArgSort, :, :].double()), 0)\n",
    "        temp = torch.cat([temp, temp, temp], 1)\n",
    "        temp[:, 0, 27, :] = torch.max(temp)\n",
    "        temp[:, 1, 27, :] = 0\n",
    "        temp[:, 2, 27, :] = 0\n",
    "        grid = make_grid(temp, nrow=30, normalize=False, range=(0, 1)).numpy()\n",
    "        writer.add_image('ssimValid', grid, epoch)\n",
    "\n",
    "\n",
    "        outTrain = outTrain.reshape((train_num, 8, 16))\n",
    "        outTrain = outTrain[:30, :, :]\n",
    "        reconstructions = capsnet.forward(torch.tensor(squash(outTrain)).to(device))[1]\n",
    "        reconstruction = reconstructions.cpu().view_as(ground_truth_train).data\n",
    "        temp = torch.cat((ground_truth_train[rndTrainArg, :, :].double(),reconstruction[rndTrainArg, :, :].double()), 0)\n",
    "        temp = torch.cat([temp, temp, temp], 1)\n",
    "        temp[:, 0, 27, :] = torch.max(temp)\n",
    "        temp[:, 1, 27, :] = 0\n",
    "        temp[:, 2, 27, :] = 0\n",
    "        grid = make_grid(temp, nrow=30, normalize=False, range=(0, 1)).numpy()\n",
    "        writer.add_image('Train', grid, epoch)\n",
    "\n",
    "        # SSIM\n",
    "        A = ground_truth_train.float().cpu().detach().numpy()\n",
    "        B = reconstruction.float().cpu().detach().numpy()\n",
    "        ssim = []\n",
    "        for i in range(len(ground_truth_train)):\n",
    "            ssim.append(compare_ssim(A[i, 0, :, :], B[i, 0, :, :]))\n",
    "        ssimArgSort = np.argsort(ssim)\n",
    "        # ssimArgSort = ssimArgSort[::-1]\n",
    "        writer.add_scalar('ssim/train', np.mean(ssim), epoch)\n",
    "        writer.add_scalar('ssimMax/train', np.max(ssim), epoch)\n",
    "        temp = torch.cat((ground_truth_train[ssimArgSort, :, :].double(),reconstruction[ssimArgSort, :, :].double()), 0)\n",
    "        temp = torch.cat([temp, temp, temp], 1)\n",
    "        temp[:, 0, 27, :] = torch.max(temp)\n",
    "        temp[:, 1, 27, :] = 0\n",
    "        temp[:, 2, 27, :] = 0\n",
    "        grid = make_grid(temp, nrow=30, normalize=False, range=(0, 1)).numpy()\n",
    "        writer.add_image('ssimTrain', grid, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
